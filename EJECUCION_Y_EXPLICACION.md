<div align="center">

# Oscar Abella  
## Prueba Tecnica Dapper   

###  Presentado a Pablo Pastrana  
#### Especializacion en ciencia de datos y analitica  
 
 

</div>

# Ejecuci√≥n del Proyecto y Explicaci√≥n para Entrevista

## Pasos para Ejecutar el Proyecto

### Paso 1: Preparar el Entorno

```bash
# 1. Navegar al directorio del proyecto
cd prueba_tecnica_DE_Dapper

# 2. Verificar que Docker est√© corriendo
docker --version
docker-compose --version

# 3. Crear directorios necesarios (si no existen)
mkdir -p logs dags plugins config configs sql
```

### Paso 2: Crear las Tablas en la Base de Datos

```bash
# Ejecutar el script SQL para crear las tablas
docker-compose exec postgres psql -U airflow -d airflow < sql/create_regulations_table.sql

# Verificar que las tablas se crearon correctamente
docker-compose exec postgres psql -U airflow -d airflow -c "\dt"
```

**Resultado esperado:** Debe mostrar las tablas `regulations` y `regulations_component`

### Paso 3: Inicializar Airflow (Solo Primera Vez)

```bash
# Opci√≥n 1: Usar Makefile
make init-airflow

# Opci√≥n 2: Manualmente
docker-compose run --rm webserver airflow db init
docker-compose run --rm webserver airflow users create \
  --username admin \
  --password admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com
```

### Paso 4: Levantar los Servicios

```bash
# Levantar todos los servicios (Postgres, Scheduler, Webserver)
docker-compose up -d

# Verificar que todos los servicios est√©n corriendo
docker-compose ps
```

**Resultado esperado:**
```
NAME                STATUS
postgres            Up
scheduler           Up
webserver           Up
```

### Paso 5: Acceder a la Interfaz de Airflow

1. Abrir navegador en: **http://localhost:8080**
2. Login:
   - Usuario: `admin`
   - Contrase√±a: `admin`

### Paso 6: Activar y Ejecutar el DAG

1. **Buscar el DAG:**
   - En la lista de DAGs, buscar `ani_regulations_scraping`
   - Verificar que aparece sin errores (icono verde)

2. **Activar el DAG:**
   - Hacer clic en el toggle (switch) a la izquierda del nombre del DAG
   - Debe cambiar a color azul (activado)

3. **Ejecutar el DAG:**
   - Hacer clic en el nombre del DAG
   - Hacer clic en el bot√≥n "Trigger DAG" (icono de play)
   - El DAG comenzar√° a ejecutarse

### Paso 7: Monitorear la Ejecuci√≥n

1. **Ver el estado general:**
   - En la vista de √°rbol o gr√°fico, ver las 3 tareas
   - Verde = √©xito, Rojo = error, Amarillo = en ejecuci√≥n

2. **Ver logs de cada tarea:**
   - Clic en la tarea `extraction`
   - Clic en "Log"
   - Debe mostrar: `‚úÖ EXTRACCI√ìN COMPLETADA` y `üìä TOTALES EXTRA√çDOS: X registros`
   
   - Repetir para `validation`:
   - Debe mostrar: `‚úÖ VALIDACI√ìN COMPLETADA`, `üìä REGISTROS ORIGINALES`, `‚ùå DESCARTES POR VALIDACI√ìN: X`
   
   - Repetir para `writing`:
   - Debe mostrar: `‚úÖ ESCRITURA COMPLETADA` y `üìù FILAS INSERTADAS: X`

### Paso 8: Verificar Datos en la Base de Datos

```bash
# Contar registros insertados
docker-compose exec postgres psql -U airflow -d airflow -c "SELECT COUNT(*) FROM regulations;"

# Ver algunos registros
docker-compose exec postgres psql -U airflow -d airflow -c "SELECT title, created_at, entity FROM regulations LIMIT 5;"
```

### Paso 9: Verificar Idempotencia (Opcional)

```bash
# Contar registros antes
docker-compose exec postgres psql -U airflow -d airflow -c "SELECT COUNT(*) FROM regulations;"

# Ejecutar el DAG nuevamente desde Airflow UI

# Contar registros despu√©s (debe ser el mismo n√∫mero si todos eran duplicados)
docker-compose exec postgres psql -U airflow -d airflow -c "SELECT COUNT(*) FROM regulations;"
```

En los logs de la tarea `writing`, debe aparecer: `Duplicados encontrados: X`

---

## Explicaci√≥n para la Entrevista (Como Data Engineer)

### Introducci√≥n

"Implement√© un pipeline de datos end-to-end para extraer, validar y almacenar normativas de la Agencia Nacional de Infraestructura (ANI) usando Apache Airflow. El proyecto sigue las mejores pr√°cticas de ingenier√≠a de datos con separaci√≥n de responsabilidades, configuraci√≥n externa e idempotencia."

### Arquitectura y Dise√±o

**1. Modularizaci√≥n:**
"Separ√© el c√≥digo en tres m√≥dulos independientes siguiendo el principio de responsabilidad √∫nica:

- **M√≥dulo de Extracci√≥n** (`extraction.py`): Contiene toda la l√≥gica de web scraping, manteniendo intacta la l√≥gica original del c√≥digo base. Incluye funciones para scrapear p√°ginas, extraer t√≠tulos, enlaces, fechas y res√∫menes, con limpieza y normalizaci√≥n de datos.

- **M√≥dulo de Validaci√≥n** (`validation.py`): Implementa un validador configurable basado en reglas YAML. Permite validar tipos de dato, expresiones regulares, longitud, rangos num√©ricos y valores permitidos. Si un campo no cumple y no es obligatorio, se pone a NULL; si es obligatorio, se descarta la fila completa.

- **M√≥dulo de Persistencia** (`persistence.py`): Maneja la conexi√≥n a base de datos y la inserci√≥n de datos. Implementa l√≥gica de detecci√≥n de duplicados para garantizar idempotencia, comparando registros por t√≠tulo, fecha de creaci√≥n, enlace externo y entidad."

**2. Configuraci√≥n Externa:**
"Las reglas de validaci√≥n est√°n en un archivo YAML (`validation_rules.yaml`), lo que permite modificar los criterios de validaci√≥n sin tocar c√≥digo. Esto facilita el mantenimiento y permite que diferentes entornos tengan diferentes reglas."

### Orquestaci√≥n con Airflow

**3. DAG de Airflow:**
"Cre√© un DAG con tres tareas en secuencia que orquestan el proceso completo:

- **Tarea de Extracci√≥n**: Scrapea las p√°ginas de ANI y extrae los datos. Los resultados se pasan a la siguiente tarea mediante XCom.

- **Tarea de Validaci√≥n**: Valida los datos extra√≠dos seg√∫n las reglas configurables. Filtra registros inv√°lidos y campos que no cumplen criterios.

- **Tarea de Escritura**: Inserta los datos validados en PostgreSQL, evitando duplicados mediante la l√≥gica de idempotencia implementada.

El DAG est√° configurado para ejecutarse cada 6 horas autom√°ticamente, con retries y manejo de errores."

### Base de Datos y Persistencia

**4. Integraci√≥n con PostgreSQL:**
"El proyecto usa la misma base de datos PostgreSQL que levanta el docker-compose de Airflow, eliminando la dependencia de AWS Secrets Manager. Las credenciales se configuran mediante variables de entorno en el docker-compose, siguiendo las mejores pr√°cticas de configuraci√≥n por entorno.

Cre√© un script SQL (`create_regulations_table.sql`) con el DDL completo para crear las tablas necesarias, incluyendo √≠ndices para optimizar las consultas de detecci√≥n de duplicados."

### Idempotencia

**5. Prevenci√≥n de Duplicados:**
"Implement√© idempotencia b√°sica reutilizando la l√≥gica existente en el c√≥digo base. La funci√≥n `insert_new_records()` consulta primero los registros existentes en la base de datos, crea claves √∫nicas combinando t√≠tulo, fecha de creaci√≥n y enlace externo, y filtra duplicados antes de insertar. Esto permite ejecutar el DAG m√∫ltiples veces sin crear registros duplicados, lo cual es cr√≠tico para pipelines de datos que pueden fallar y necesitan re-ejecutarse."

### Logs y Observabilidad

**6. Logging Claro:**
"Implement√© logs estructurados que muestran claramente:
- **TOTALES EXTRA√çDOS**: Cantidad de registros obtenidos del scraping
- **DESCARTES POR VALIDACI√ìN**: Registros descartados por no cumplir reglas
- **FILAS INSERTADAS**: Registros finalmente insertados en la base de datos

Esto facilita el monitoreo y debugging del pipeline."

### Manejo de Errores

**7. Robustez:**
"Cada etapa del pipeline tiene manejo de errores apropiado. Si la validaci√≥n falla, el proceso contin√∫a con los datos sin validar (fallback). Si la escritura falla, se captura el error y se reporta claramente en los logs. Esto asegura que el pipeline sea resiliente a errores parciales."

### Puntos Clave para Destacar

**‚úÖ Separaci√≥n de Responsabilidades:**
"Cada m√≥dulo tiene una responsabilidad √∫nica y clara, facilitando el mantenimiento y testing."

**‚úÖ Configuraci√≥n Externa:**
"Las reglas de validaci√≥n est√°n en YAML, permitiendo cambios sin modificar c√≥digo."

**‚úÖ Idempotencia:**
"El pipeline puede ejecutarse m√∫ltiples veces sin crear duplicados, esencial para pipelines de producci√≥n."

**‚úÖ Observabilidad:**
"Logs claros que muestran m√©tricas clave en cada etapa del proceso."

**‚úÖ Operabilidad:**
"README conciso con instrucciones claras, variables de entorno configuradas, y estructura de repositorio entendible."

### Respuesta a Preguntas Comunes

**P: ¬øPor qu√© separaste en m√≥dulos?**
R: "Para facilitar el mantenimiento, testing y reutilizaci√≥n. Cada m√≥dulo puede evolucionar independientemente y es m√°s f√°cil identificar y corregir problemas."

**P: ¬øC√≥mo manejas los duplicados?**
R: "Antes de insertar, consulto los registros existentes y creo claves √∫nicas combinando t√≠tulo, fecha y enlace. Comparo los nuevos registros con los existentes usando sets para eficiencia O(1), y filtro duplicados antes de la inserci√≥n."

**P: ¬øQu√© pasa si falla una etapa?**
R: "Cada etapa tiene manejo de errores. Si la validaci√≥n falla, contin√∫o con los datos sin validar. Si la escritura falla, capturo el error y lo reporto en logs. El DAG tiene retries configurados para recuperarse de errores temporales."

**P: ¬øC√≥mo se puede escalar esto?**
R: "El dise√±o modular permite escalar horizontalmente. Podr√≠amos paralelizar la extracci√≥n de m√∫ltiples p√°ginas, usar un validador distribuido, o implementar inserci√≥n en batch m√°s grande. La separaci√≥n de m√≥dulos facilita estas optimizaciones."

### Cierre

"El proyecto demuestra mi capacidad para dise√±ar pipelines de datos robustos, modulares y operables, siguiendo las mejores pr√°cticas de la industria. Est√° listo para producci√≥n con idempotencia, logging claro y manejo de errores apropiado."

---

## Comandos R√°pidos de Referencia

```bash
# Levantar todo
docker-compose up -d

# Ver logs del scheduler
docker-compose logs -f scheduler

# Ver logs del webserver
docker-compose logs -f webserver

# Ejecutar SQL directamente
docker-compose exec postgres psql -U airflow -d airflow

# Reiniciar servicios
docker-compose restart

# Detener todo
docker-compose down

# Detener y limpiar vol√∫menes
docker-compose down --volumes
```

---

## ‚úÖ Checklist de Verificaci√≥n Pre-Entrevista

- [ ] Proyecto ejecuta sin errores
- [ ] DAG aparece en Airflow
- [ ] Las 3 tareas se ejecutan correctamente
- [ ] Logs muestran totales, descartes e insertados
- [ ] Datos se insertan en la BD
- [ ] Idempotencia funciona (no duplica)
- [ ] README est√° completo y claro
- [ ] Puedo explicar cada componente del proyecto

By Oscar Abella